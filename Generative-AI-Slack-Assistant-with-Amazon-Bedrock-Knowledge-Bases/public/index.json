[
{
	"uri": "//localhost:1313/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "This project introduces an AI-powered assistant chatbot designed for integration with Slack, built upon the managed RAG (Retrieval-Augmented Generation) capabilities of Amazon Bedrock. At its core, the solution is structured to provide intelligent, context-aware responses to user queries by tapping into an organization\u0026rsquo;s internal knowledge base. The chatbot is triggered via Slack\u0026rsquo;s custom slash command /ask-ai, using the Slack Bolt Python SDK to handle event processing, authentication, and command execution.\nOnce a user submits a question in Slack, the request is routed to an AWS Lambda function via API Gateway. This Lambda function interacts directly with the Amazon Bedrock Knowledge Base service, which orchestrates the entire RAG pipeline. The chatbot retrieves relevant content from the knowledge base through vector similarity search powered by Amazon OpenSearch Serverless, which stores and indexes document embeddings that were generated using Amazon Titan Text Embeddings v2. These embeddings support multilingual processing, particularly optimized for Vietnamese and English, allowing users to interact naturally in both languages.\nAfter the appropriate content is retrieved from the vector store, it is passed along with the original user query to Claude 3 Sonnet, a lightweight and cost-effective large language model hosted within Amazon Bedrock. This model generates the final response that is returned to the Slack user.\nThe entire system is built using a serverless architecture, leveraging AWS Lambda for compute, API Gateway for routing, S3 for document storage, and Bedrock for managed AI services. All components are defined and deployed using the AWS Cloud Development Kit (CDK), which allows infrastructure to be managed as code. This enables full automation, scalability, and maintainability while keeping operational overhead low.\nDesigned with flexibility and extensibility in mind, this chatbot can be adapted for a wide range of scenarios such as internal documentation access, frequently asked questions (FAQs), customer service support, or enterprise-level knowledge sharing. The use of Amazon Bedrock’s managed orchestration combined with serverless infrastructure results in a powerful AI assistant that is secure, scalable, cost-efficient, and highly responsive to end-user needs.\n"
},
{
	"uri": "//localhost:1313/1-introduction/1.1-overview/",
	"title": "Solution overview",
	"tags": [],
	"description": "",
	"content": "This solution integrates Slack with Amazon Bedrock and its Knowledge base to enable business users to interact with a generative AI assistant directly within Slack. The system processes user queries asynchronously, performs safety checks using Amazon Bedrock Guardrails, and retrieves relevant knowledge from a vectorized database built on Amazon OpenSearch, populated from documents stored in Amazon S3.\nThe backend is fully serverless, using AWS Lambda and API Gateway to manage Slack interactions, and relies on Amazon Bedrock for prompt handling and intelligent responses. This architecture enables secure, scalable, and compliant AI-powered support for enterprise users in real time.\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Building Trustworthy AI with Amazon Bedrock Guardrails",
	"tags": [],
	"description": "",
	"content": "Building Trustworthy AI with Amazon Bedrock Guardrails Workshop Overview Guardrails for Amazon Bedrock "
},
{
	"uri": "//localhost:1313/2-prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " To get started, go to Slack.com. Click to Create a new workspace.\nEnter your email to sign in\nClick Create a Workspace\nFinally create a slack workspace.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/3.1-create_slackapp/",
	"title": "Create slack app",
	"tags": [],
	"description": "",
	"content": "Create slack app Access to api.slack.com/apps. Click Create new app -\u0026gt; From scratch\nEnter your App name, Pick a workspace to develop your app in and click Create App.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/3.2-oauthpermissions/",
	"title": "Create slash commands",
	"tags": [],
	"description": "",
	"content": "Create slash commands Choose Slash Commands. Then click Create new command.\nEnter your information: command name, Request URL, Short Decription.\nNote: The current URL is only temporary, when deployed, we will use the API URL to replace it.\nCreate slash commands successfully.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/",
	"title": "Slack App Setup",
	"tags": [],
	"description": "",
	"content": "A Slack app is a tool or integration that extends the functionality of Slack: it adds new features, automates tasks, integrates with external services, or enhances the user experience. A Slack app allows you to do more within Slack than just chat. With the Slack platform, individual and enterprise developers alike can create apps that integrate directly with the tools teams already use, whether that\u0026rsquo;s connecting a CRM, managing project boards, or sending automated alerts.\nWe know our platform is deep and wide, and possibly a little intimidating as a result. It\u0026rsquo;s okay to not know where to start.\nIf you want to take it slow, this guide on designing your app is a little light reading on how to define the look and feel of your app.\nIf you\u0026rsquo;d rather stop the chitchat and get into it, build an app with the Quickstart guide. If you\u0026rsquo;re just looking to get a token to call the Web API methods, completing the first three steps of the Quickstart will get you there.\n"
},
{
	"uri": "//localhost:1313/4-security/4.1-secret-manager/",
	"title": "Secrets Manager",
	"tags": [],
	"description": "",
	"content": " Access to Secret console\nGet token:\nCreate bot-token5\nGet Signing secret\nCreate signing-secret5\n"
},
{
	"uri": "//localhost:1313/4-security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "Security is a foundational element of this chatbot system, and it is implemented through a dual-layered approach that combines the strengths of AWS Secrets Manager and AWS Systems Manager Parameter Store to ensure the secure handling of sensitive credentials and configurations throughout the application lifecycle.\nThe primary function of AWS Secrets Manager in this project is to securely store and manage confidential credentials, particularly those required for the Slack bot\u0026rsquo;s authentication. These include the Slack Bot User OAuth Token, which is essential for programmatic access to the Slack API, and the Slack Signing Secret, which is used to validate that incoming requests to the application originate from Slack. Secrets Manager ensures these values are encrypted at rest using AWS-managed keys and supports automatic secret rotation, which enhances long-term security and reduces the risk of credential exposure. Furthermore, all access to stored secrets is tightly controlled through IAM policies, allowing only explicitly authorized roles and services to retrieve them.\nComplementing Secrets Manager is the AWS Systems Manager Parameter Store, which plays a key role in configuration management and runtime secret resolution. Instead of Lambda functions directly accessing secrets, the architecture leverages Parameter Store to create references to the secrets stored in Secrets Manager. Using the native {{resolve:secretsmanager:\u0026hellip;}} syntax, Lambda functions can dynamically fetch the latest values at execution time. This approach avoids hardcoding sensitive data in source code or environment variables, and makes deployments cleaner and safer. The use of Standard-tier parameters within SSM also ensures that the configuration layer remains cost-effective.\nThis design provides several security benefits. First, it enables a separation of concerns, where Secrets Manager handles encryption and storage, while Parameter Store manages controlled distribution. Second, credentials are never exposed in plain text or stored within the codebase, reducing attack vectors. Third, this setup supports dynamic runtime resolution, meaning credentials are always up to date when Lambda functions run. Fourth, auditing and monitoring are enabled for both Secrets Manager and Parameter Store, providing complete traceability for compliance. Lastly, it follows the principle of least privilege access, as Lambda functions require permission to read from SSM, but do not need direct access to Secrets Manager.\nTogether, these services establish a robust security architecture that ensures sensitive Slack credentials remain protected while still being seamlessly accessible to the AI chatbot application at runtime.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/3.3-slash_commands/",
	"title": "Get OAuth Tokens",
	"tags": [],
	"description": "",
	"content": " Choose OAuth \u0026amp; Permissions, choose install to Your-workspace.\nYou will see OAuth Tokens.\n"
},
{
	"uri": "//localhost:1313/4-security/4.2-systems-manager/",
	"title": "Systems Manager",
	"tags": [],
	"description": "",
	"content": " Access to SSM console\nCreate new Parameter for bot-token5\nCreate new Parameter for signing-secret5\nARN\n"
},
{
	"uri": "//localhost:1313/5-opensearch/5.1-collection/",
	"title": "collection",
	"tags": [],
	"description": "",
	"content": " Access to Amazon Opensearch Service Console.\nScroll down and select Data access policies and click create access policy\nEnter Access policy name -\u0026gt; choose JSON\nEnter the following code and Create\n[ { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;index\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;index/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;/*\u0026#34;], \u0026#34;Permission\u0026#34;: [\u0026#34;aoss:*\u0026#34;] }, { \u0026#34;ResourceType\u0026#34;: \u0026#34;collection\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;collection/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;\u0026#34;], \u0026#34;Permission\u0026#34;: [\u0026#34;aoss:*\u0026#34;] } ], \u0026#34;Principal\u0026#34;: [\u0026#34;arn:aws:iam::\u0026lt;YOUR-ACCOUNT-ID\u0026gt;:root\u0026#34;] } ] Create networks policy\nName Network policy name\nEnter the following code and Create\n[ { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;collection\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;collection/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;\u0026#34;] }, { \u0026#34;ResourceType\u0026#34;: \u0026#34;dashboard\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;collection/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;\u0026#34;] } ], \u0026#34;AllowFromPublic\u0026#34;: true } ] Create Collection Select Collections\nSelect Create Collection\nEnter Collection name and select Collection type: Vector search\nDefault and create. Note:\nCollection ARN Opensearch endpoint\n"
},
{
	"uri": "//localhost:1313/5-opensearch/",
	"title": "Set up opensearch",
	"tags": [],
	"description": "",
	"content": "In the context of this AI chatbot solution, Amazon OpenSearch Serverless functions as the centralized vector database, acting as the memory engine that drives the semantic retrieval capabilities of the Retrieval-Augmented Generation (RAG) workflow.\nWhen documents are first ingested into the system, they are processed and transformed into high-dimensional vector embeddings using the Amazon Titan Text Embeddings v2 model. These embeddings, which support semantic understanding across multiple languages including Vietnamese and English, are stored in a dedicated vector collection within OpenSearch Serverless. The vector collection, identified by the name slack-bedrock-vector-db, offers fully managed, serverless, and auto-scaling storage of embedding vectors, eliminating the need for manual cluster management.\nTo facilitate efficient retrieval, these stored vectors are indexed using a vector index named slack-bedrock-os-index, which enables high-performance similarity search using k-nearest neighbor (k-NN) algorithms. The OpenSearch index also retains metadata associated with the original document chunks, such as file names or section titles, ensuring that retrieval results are both contextually accurate and traceable.\nDuring a typical query interaction, when a user submits a question via Slack, the input is embedded using the same Titan v2 model. This query embedding is sent to OpenSearch, where a vector similarity search is conducted against the stored embeddings. The most relevant chunks of documents are then retrieved and returned to the RAG system to serve as context for final response generation.\nThe serverless nature of OpenSearch ensures that it scales automatically with traffic volume and operates on a pay-per-use pricing model, helping keep costs predictable and aligned with usage. The architecture requires no manual provisioning or maintenance of nodes, which significantly simplifies operations. With sub-second response times, the system supports real-time querying, providing users with instant, relevant information.\nSecurity is also embedded into the design. OpenSearch Serverless integrates with IAM-based access control and supports encryption both at rest and in transit. Through the BedrockOSSPolicyForKnowledgeBase policy, fine-grained access permissions are enforced, ensuring that only authorized components can access or query the vector database.\nUltimately, OpenSearch Serverless functions as the persistent memory layer of the AI assistant, enabling it to recall and reference vast amounts of organizational knowledge with semantic precision.\n"
},
{
	"uri": "//localhost:1313/5-opensearch/5.2-vector_index/",
	"title": "Vector index",
	"tags": [],
	"description": "",
	"content": "Create user Create a user bedrock-chatbot-deployer with policy AdministratorAccess\nCreate a tag with Key and Value are access key\nCreate index Access to Postman with: URL is Collection endpoint/\u0026lt;Index_name\u0026gt;\nAuth type: AWS Signature\nAccessKey and SecretKey\nRegion: us-east-1\nService name: aoss\nHeader:\nContent type: Application/json\nBody:\nraw\nEnter code following { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;knn\u0026#34;: true, \u0026#34;knn.algo_param.ef_search\u0026#34;: 512 } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;bedrock-knowledge-base-default-vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_vector\u0026#34;, \u0026#34;dimension\u0026#34;: 1024, \u0026#34;method\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hnsw\u0026#34;, \u0026#34;engine\u0026#34;: \u0026#34;faiss\u0026#34;, \u0026#34;space_type\u0026#34;: \u0026#34;l2\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;ef_construction\u0026#34;: 512, \u0026#34;m\u0026#34;: 16 } } }, \u0026#34;AMAZON_BEDROCK_METADATA\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: false }, \u0026#34;AMAZON_BEDROCK_TEXT_CHUNK\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;x-amz-bedrock-kb-data-source-id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;x-amz-bedrock-kb-document-page-number\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;x-amz-bedrock-kb-source-uri\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true } } } } Click Send. Correct configuration will get response\nAn index will be created with Vector field name and Metadata\n"
},
{
	"uri": "//localhost:1313/6-bedrock_setup/",
	"title": "Bedrock setup",
	"tags": [],
	"description": "",
	"content": "In this workshop, we use a knowledge base to retrieve information provided from S3. The data from S3 is split into smaller segments (document chunks) and converted into vectors using an embeddings model. These vectors are then stored in a vector store, specifically Amazon OpenSearch Serverless.\nWhen a user submits a question, the system performs semantic search against the vector store to retrieve relevant text segments (context). This contextual information is incorporated into prompt augmentation to provide the necessary background for the Large Language Model (LLM). Finally, the LLM generates an accurate and contextually appropriate response based on the knowledge base.\nAt the heart of this AI chatbot system lies Amazon Bedrock, which serves both as the foundation model hosting platform and the managed orchestration engine for the entire RAG (Retrieve-and-Generate) pipeline. Bedrock enables the seamless combination of foundation models, vector-based retrieval, and knowledge base indexing into a unified, scalable workflow.\nThe chatbot relies on two key models provided by Bedrock. For text embedding, it utilizes Amazon Titan Text Embeddings v2, a multilingual embedding model optimized for languages like Vietnamese and English, capable of generating rich semantic representations of documents and queries. For text generation, it employs Claude 3 Haiku, a lightweight and cost-efficient model that produces natural, fluent, and contextually appropriate responses.\nThe Amazon Bedrock Knowledge Base component acts as the central RAG orchestrator. It ingests documents stored in S3 (via a designated data source), generates vector embeddings using Titan v2, and stores them in OpenSearch Serverless. At runtime, when a user submits a query, the RetrieveAndGenerate API of Bedrock is invoked. This single API call handles all the complexity: embedding the query, retrieving the most relevant document chunks via vector similarity search, passing the content to Claude 3 Haiku, and returning the generated response.\nA crucial layer of security and response control is handled by Bedrock Guardrails, configured under the name slack-bedrock-guardrail. This feature is responsible for content filtering, ensuring that responses do not contain sensitive, inappropriate, or harmful language, including topics related to violence, hate, or misconduct. It also offers protection against prompt injection attacks and enables custom error messaging, ensuring responses meet safety and compliance standards required in enterprise environments.\nFrom a technical operations perspective, Bedrock requires no infrastructure provisioning, as it offers serverless access to foundation models with automatic scalability, maintenance, and updates. This significantly reduces the burden of managing ML infrastructure. The solution is also region-optimized for ap-southeast-1, ensuring latency and cost efficiency for users in Southeast Asia.\nAccess to Bedrock is strictly controlled through IAM roles (e.g., bedrockExecutionRole), and all operations are logged to support auditing, governance, and enterprise-grade security compliance.\nIn essence, Amazon Bedrock functions as the “brain” of the AI chatbot, coordinating embedding, retrieval, content generation, and moderation — all under a fully managed, secure, and scalable AI service platform.\n"
},
{
	"uri": "//localhost:1313/6-bedrock_setup/6.2-guardrails/",
	"title": "Set up guardrails",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "//localhost:1313/6-bedrock_setup/6.1-model_access/",
	"title": "Set up model access",
	"tags": [],
	"description": "",
	"content": " Access to Amazon Bedrock Console.\nScroll down to the bottom of the navigation sidebar and choose Model access.\nChoose Modify model access.\nTick into the box of the Models, what you want to enable. In this workshop, i use Titan Text Embeddings V2 and Claude 3 sonnet. Scroll down to the bottom and choose Next\nBecause Titan Text Embeddings V2 was access granted before, there is only Claude 3 sonnet left. Choose Submit and after a few minutes model access will be enabled.\nFinally, both Titan Text Embeddings V2 and Claude 3 sonnet are access granted.\n"
},
{
	"uri": "//localhost:1313/7-lambda_implementation/",
	"title": "Lambda implementation",
	"tags": [],
	"description": "",
	"content": "The Lambda function in this code acts as an intermediary between Slack and Amazon Bedrock Knowledge Base. When a user enters a Slash command in Slack (e.g., /ask-ai), the event is sent to the Lambda function. Within the first 3 seconds, the Lambda must acknowledge the command using the ack function to avoid a timeout error from Slack—this is handled by the respond_to_slack_within_3_seconds function. After that, the Lambda proceeds to process the main logic by calling Bedrock\u0026rsquo;s RetrieveAndGenerate API to retrieve relevant information and generate a response from the Knowledge Base, which is then sent back to the user in Slack.\nIn addition to responding to user queries, the Lambda function is responsible for securely initializing and managing required AWS services such as retrieving secrets from AWS Secrets Manager and parameters from SSM Parameter Store. It also sets up configuration values like model ID, knowledge base ID, and guardrail settings. This ensures that the connection between Slack and Bedrock is correctly established and that user queries are processed safely and accurately. In short, this Lambda function acts both as an orchestrator and executor of the entire interaction flow between Slack and AWS\u0026rsquo;s intelligent answering system.\n"
},
{
	"uri": "//localhost:1313/7-lambda_implementation/7.1-lambda_role/",
	"title": "Lambda role",
	"tags": [],
	"description": "",
	"content": "Create a role named BedrockExecutionRole8888: Policy AWSLambdaBasicExecutionRole:\nAdd a custom policy named BedrockExecutionPolicy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:Retrieve\u0026#34;, \u0026#34;bedrock:RetrieveAndGenerate\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;YOUR-KNOWLEDGEBASE-ARN\u0026gt;\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:GetParameter\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;\u0026lt;YOUR-SSM-ARN-1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;YOUR-SSM-ARN-1\u0026gt;\u0026#34; ] }, { \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:InvokeFunction\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;YOUR-LAMBDA-FUNCTION-ARN\u0026gt;\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:ApplyGuardrail\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:us-east-1:\u0026lt;YOUR-ACCOUNT-ID\u0026gt;:guardrail/\u0026lt;YOUR-GUARDRAIL-ID\u0026gt;*\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:GetSecretValue\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;\u0026lt;YOUR-SECRET-BOT-TOKEN-ARN\u0026gt;\u0026#34;, \u0026#34;\u0026lt;YOUR-SECRET-SIGNING-SECRET-ARN\u0026gt;\u0026#34; ] } ] } "
},
{
	"uri": "//localhost:1313/6-bedrock_setup/6.3-knowledge_base/",
	"title": "Set up knowledge base",
	"tags": [],
	"description": "",
	"content": "S3 configuration Create a new bucket.\nConfigure on Bedrock Access to Amazon Bedrock Console.\nScroll down to the bottom of the navigation sidebar and choose Knowledge base.\nChoose Create and choose Knowledge Base with vector store.\nSet up knowledge base with vector store.\nEnter knowledge base name and create a new role\nData source type will be S3\nSelect S3 to store data\nSelect Embeddings model\nVector store is Amazon Opensearch Serverless\nConfiguring the Vector Store with what was created in the previous steps is the set up opensearch preparation step\nKnowledge Base will be created successfully.\nRole for knowledge base will have policies\nStore dato into Knowledge Base Upload data that chatbot will use with s3. In this session, we use postgresql-16-US.pdf\nAccess to data source in bedrock knowledge base and sync it\n"
},
{
	"uri": "//localhost:1313/7-lambda_implementation/7.2-config_code/",
	"title": "Configuration and code",
	"tags": [],
	"description": "",
	"content": " Create a lambda named BedrockKBSlackbotFunction5\nAttach role named BedrockExecutionRole8888 and Create\nUp load a file .zip named BedrockKBSlackbotFunction.zip\nChange Hanlder:\nChange Memory and Timeout\nAdd some Environment variables\n"
},
{
	"uri": "//localhost:1313/8-api_gateway/",
	"title": "Connect and chat",
	"tags": [],
	"description": "",
	"content": "API GATEWAY URL Access to API console\nCreate Rest API and name\nCreate resource named slack\nCreate another source named ask-ai\nCreate method for ask-ai\nSettings with method:\nMethod: POST Lambda proxy integration: enable Lambda function: BedrockKbSlackbotFunction5\nDeploy with new stage named prod\nGet the Invoke URL and replace it with the Request URL box in Slack. Choose Save\nAccess to Slash commands again and replace Request URL. Finally, click Save\nTESTING Use Case 1: Chatbot successfully retrieves data from the Knowledge Base\nUse Case 2: Chatbot cannot answer because the data is not present in the PDF\nUse Case 3: Chatbot cannot respond due to sensitive content blocked by AWS Bedrock Guardrails\n"
},
{
	"uri": "//localhost:1313/9-clear-resources/",
	"title": "Clear resources",
	"tags": [],
	"description": "",
	"content": "Clear resource by following steps S3\nKnowledge Base\nOpensearch Indexes\nCollections\nData access policy\nNetwork policy\nAWS Secret Managers\nAWS Systems Manager\nLambda\nAPI gateway\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]