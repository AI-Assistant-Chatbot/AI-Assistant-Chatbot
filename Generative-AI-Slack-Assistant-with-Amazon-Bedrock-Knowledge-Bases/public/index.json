[
{
	"uri": "//localhost:1313/1-introduction/",
	"title": "Introduction to Slackbot Assistant with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Introduction Welcome to this comprehensive workshop on building a Generative AI Slack Assistant with Amazon Bedrock Knowledge Bases! In this hands-on experience, you\u0026rsquo;ll learn to create an intelligent AI assistant that transforms how your organization accesses and utilizes knowledge, making information instantly available through natural language conversations in Slack.\nThe Challenge We\u0026rsquo;re Addressing Modern organizations face a critical productivity challenge: information silos. Valuable knowledge is scattered across documents, wikis, policies, and databases, making it difficult for employees to find answers quickly. Traditional search methods often fail to understand context and intent, leading to:\nTime waste searching through multiple documents Inconsistent answers from different team members Knowledge gaps when experts are unavailable Reduced productivity due to information friction Our Solution Approach This workshop demonstrates how to transform static organizational knowledge into an intelligent, conversational assistant that integrates seamlessly with existing workflows through Slack.\nWorkshop Objectives By completing this workshop, you will achieve the following learning outcomes:\nPrimary Objectives Master RAG Architecture\nUnderstand Retrieval-Augmented Generation principles Implement vector-based semantic search Design efficient document ingestion pipelines Build Production-Ready AI Systems\nDeploy scalable serverless architecture Implement proper error handling and monitoring Configure auto-scaling and cost optimization Integrate Enterprise Security\nConfigure Amazon Bedrock Guardrails Implement PII detection and anonymization Set up content filtering and safety measures Develop Slack Bot Applications\nUse Slack Bolt framework for Python Handle asynchronous message processing Implement proper authentication and permissions Technical Skills You\u0026rsquo;ll Gain Amazon Bedrock integration and model management OpenSearch Serverless vector database operations AWS Lambda serverless function development API Gateway REST API configuration Infrastructure as Code using AWS CDK Slack API development and webhook handling SlackBot Architecture Architecture Flow Diagram graph TB\rA[Slack User] --\u0026gt;|/ask-aws command| B[API Gateway]\rB --\u0026gt; C[Lambda Function]\rC --\u0026gt; D[Bedrock Knowledge Base]\rD --\u0026gt; E[OpenSearch Serverless]\rD --\u0026gt; F[Claude 3 Sonnet]\rG[S3 Documents] --\u0026gt;|Ingestion| D\rF --\u0026gt; H[Bedrock Guardrails]\rH --\u0026gt;|Filtered Response| I[Slack Channel]\rstyle A fill:#e1f5fe\rstyle D fill:#f3e5f5\rstyle F fill:#fff3e0\rstyle H fill:#ffebee Key Components Explained Component Purpose Benefits Slack Interface User interaction layer Familiar interface, no training required API Gateway HTTP endpoint management Secure, scalable API access Lambda Function Business logic processing Serverless, cost-effective compute Bedrock Knowledge Base RAG orchestration Fully managed, no infrastructure OpenSearch Serverless Vector storage and search Auto-scaling, semantic search Bedrock Guardrails Content safety and filtering Enterprise-grade security What You\u0026rsquo;ll Build 1. Intelligent Slack Assistant\nNatural Language Processing: Ask questions in plain English Contextual Responses: AI-powered answers with source citations Multi-Document Search: Query across your entire knowledge base Real-Time Processing: Sub-3-second response acknowledgment 2. Enterprise Security Features\nContent Filtering: Block inappropriate or harmful content PII Protection: Automatic detection and anonymization Prompt Injection Defense: Prevent malicious prompt manipulation Access Control: Workspace-based permissions 3. Production-Ready Infrastructure\nAuto-Scaling: Handle varying workloads automatically Monitoring: CloudWatch integration for observability Error Handling: Graceful degradation and user feedback Cost Optimization: Pay-per-use serverless model Real-World Applications This solution pattern enables numerous enterprise use cases:\n1. Customer Support\nInstant answers from product documentation Consistent responses across support teams Reduced ticket volume and response times 2. Human Resources\nEmployee self-service for policy questions Onboarding assistance and training materials Benefits and procedure clarification 3. Technical Documentation\nEngineering knowledge base access API documentation and troubleshooting guides Best practices and coding standards 4. Compliance \u0026amp; Legal\nRegulatory requirement queries Policy interpretation and guidance Audit preparation and documentation Technology Stack Deep Dive 1. Amazon Bedrock Ecosystem\nFoundation Models: Claude 3 Sonnet for text generation Embeddings: Titan Text Embeddings V2 for vector creation Knowledge Bases: Managed RAG workflow orchestration Guardrails: Content safety and filtering mechanisms 2. Supporting AWS Services\nLambda: Event-driven serverless compute API Gateway: RESTful API management S3: Document storage and versioning OpenSearch Serverless: Vector database operations CloudWatch: Logging, monitoring, and alerting Secrets Manager: Secure credential storage 3. Development Framework\nSlack Bolt for Python: Simplified bot development AWS CDK: Infrastructure as Code deployment Python 3.12+: Modern language features and performance Workshop Learning Path graph LR\rA[Setup \u0026amp; Prerequisites] --\u0026gt; B[Knowledge Base Foundation]\rB --\u0026gt; C[Bedrock Integration]\rC --\u0026gt; D[Slack Bot Development]\rD --\u0026gt; E[Security \u0026amp; Guardrails]\rE --\u0026gt; F[Production Deployment]\rF --\u0026gt; G[Testing \u0026amp; Validation] Success Metrics Upon workshop completion, you\u0026rsquo;ll have achieved:\n1. Functional Deliverables\nWorking Slack bot responding to natural language queries Deployed AWS infrastructure with proper security Integrated knowledge base with your documents Monitoring and alerting configuration 2. Measurable Outcomes\nResponse Time: \u0026lt; 3 seconds for query acknowledgment Accuracy: Relevant answers with source attribution Security: Zero inappropriate content in responses Scalability: Handle 100+ concurrent users 3. Knowledge Transfer\nUnderstanding of RAG architecture principles Hands-on experience with Amazon Bedrock services Production deployment best practices Enterprise AI security implementation Next Steps Now that you understand the workshop objectives and architecture, you\u0026rsquo;re ready to begin the hands-on implementation. The journey ahead will transform you from an AI curious professional to someone capable of deploying production-grade generative AI solutions.\nLet\u0026rsquo;s start building the future of workplace productivity! üöÄ\nContinue to: Module 2 - Environment Setup\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Building a Generative AI Slackbot Assistant with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Building a Generative AI Slackbot Assistant with Amazon Bedrock Quick Summary Build a production-ready Slack bot powered by Amazon Bedrock Knowledge Bases that intelligently answers questions about your organization\u0026rsquo;s documents using advanced AI and Retrieval-Augmented Generation (RAG).\nWhat you\u0026rsquo;ll create:\nAI-powered Slack bot with /ask-aws commands Enterprise-grade RAG system using Amazon Bedrock Secure content filtering with Bedrock Guardrails Scalable serverless architecture on AWS Duration Total Workshop Time: 3-4 hours\nModule Duration Focus Area Setup \u0026amp; Environment 45 min AWS configuration, Slack app creation Knowledge Base 75 min Document ingestion, vector database setup Bot Development 90 min Lambda functions, API Gateway integration Security \u0026amp; Testing 30 min Guardrails, validation, monitoring Target Audience 1. Primary Audience\nSolutions Architects looking to implement AI-powered assistants Developers interested in serverless AI applications DevOps Engineers deploying production AI systems Technical Leaders evaluating enterprise AI solutions 2. Secondary Audience\nProduct Managers understanding AI integration possibilities IT Professionals implementing workplace productivity tools Data Engineers working with unstructured data processing Prerequisite Knowledge 1. Required Skills\nAWS Fundamentals: Basic understanding of Lambda, S3, IAM, API Gateway Python Programming: Intermediate level (functions, APIs, error handling) REST APIs: Understanding of HTTP methods and JSON responses Command Line: Comfortable with terminal/command prompt operations 2. Helpful Background\nServerless Architecture: Experience with event-driven computing AI/ML Concepts: Basic understanding of embeddings and vector search Slack Development: Familiarity with Slack apps and webhooks Infrastructure as Code: Experience with CloudFormation or CDK 3. Technical Requirements\nAWS Account with administrative access Slack Workspace with app installation permissions Development Environment: Python 3.12+, AWS CLI, code editor Internet Connection: For downloading dependencies and accessing AWS services Cost Breakdown 1. Workshop Completion Cost: ~$5-10\nAWS Service Usage During Workshop Estimated Cost AWS Lambda 1,000 invocations $0.20 API Gateway 1,000 requests $3.50 Amazon Bedrock 100K tokens (Claude 3 + Titan) $3.00 OpenSearch Serverless 1 OCU √ó 4 hours $0.96 S3 Storage 10GB document storage $0.23 CloudWatch Logs Basic logging $0.50 2. Production Monthly Estimates\nSmall Team (10 users, 500 queries/month): ~$45\nMedium Team (50 users, 2,500 queries/month): ~$175\nLarge Team (200 users, 10,000 queries/month): ~$650\n3. Cost Optimization Tips\nUse OpenSearch Serverless auto-scaling Implement Lambda provisioned concurrency only if needed Monitor Bedrock token usage with CloudWatch Set up billing alerts for cost control Workshop Value Proposition 1. Immediate Benefits\nHands-on Experience with cutting-edge AWS AI services Production-Ready Code you can deploy in your organization Best Practices for enterprise AI implementation Security Patterns for responsible AI deployment 2. Long-term Skills\nRAG Architecture design and implementation Serverless AI application development Enterprise Integration patterns for AI systems Cost Optimization strategies for AI workloads 3. Business Impact\nReduced Support Tickets through self-service knowledge access Improved Productivity with instant information retrieval Enhanced Collaboration through intelligent Slack integration Scalable Foundation for additional AI use cases Ready to begin? Ensure you meet the prerequisites above, then proceed to the 1. Introduction to Slackbot Assistant with Amazon Bedrock for detailed learning objectives and architecture overview.\n"
},
{
	"uri": "//localhost:1313/2-environment-setup/",
	"title": "Environment Setup",
	"tags": [],
	"description": "",
	"content": "Environment Setup Overview Before building your Generative AI Slack Assistant, you need to set up the required environments and tools. This module will guide you through creating a Slack workspace, configuring AWS services, and preparing your development environment.\nPrerequisites Checklist Ensure you have the following before proceeding:\n‚úÖ AWS Account with administrative access ‚úÖ Email address for Slack workspace creation ‚úÖ Python 3.12+ installed on your local machine ‚úÖ AWS CLI installed and configured ‚úÖ Code editor (VS Code recommended) Module Learning Objectives By the end of this module, you will have:\nCreated a Slack workspace for testing the bot Configured AWS CLI with proper credentials Enabled Amazon Bedrock models in your AWS account Set up your development environment with required dependencies Verified all prerequisites for the workshop Create Slack Workspace 1. Navigate to Slack\nGo to Slack.com and click Create a new workspace. 2. Sign Up Process\nEnter your email to sign in 3. Click Create a Workspace\nComplete Workspace Setup 1. Complete the workspace creation process by providing:\nWorkspace name (e.g., \u0026ldquo;AI-Assistant-Workshop\u0026rdquo;) Channel name (e.g., \u0026ldquo;#ai-assistant\u0026rdquo;) Team member invitations (optional for workshop) Tip: Keep your workspace name simple and related to the workshop. You\u0026rsquo;ll use this workspace to test your AI assistant.\nAWS Account Configuration 1. Verify AWS Account Access\nLog in to AWS Console\nNavigate to AWS Console Ensure you have administrative access or the following permissions: Amazon Bedrock full access Lambda full access API Gateway full access S3 full access OpenSearch Serverless access 2. Configure AWS CLI\nInstall AWS CLI (if not already installed)\n# Windows msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi # macOS curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / # Linux curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install 3. Configure AWS credentials\naws configure Enter the following when prompted:\nAWS Access Key ID: Your access key AWS Secret Access Key: Your secret key Default region name: us-east-1 (recommended for Bedrock) Default output format: json 4. Verify configuration\naws sts get-caller-identity Important: Use us-east-1 region for this workshop as it has the best model availability for Amazon Bedrock.\nEnable Amazon Bedrock Models 1. Access Bedrock Console\nNavigate to Amazon Bedrock Go to Amazon Bedrock Console Ensure you\u0026rsquo;re in the us-east-1 region 3.2 Request Model Access Enable required models Click Model access in the left navigation Click Modify model access or Enable specific models Select the following models: ‚úÖ Amazon Titan Text Embeddings V2 (for vector embeddings) ‚úÖ Anthropic Claude 3 Sonnet (for text generation) Submit access request Click Next ‚Üí Submit Wait for approval (usually instant for these models) 3.3 Verify Model Access Confirm access granted Check that both models show Access granted status This may take a few minutes Note: Model access is typically granted immediately for Titan and Claude models. If you encounter issues, ensure your AWS account is in good standing.\nStep 4: Development Environment Setup 4.1 Python Environment Verify Python installation\npython --version # Should show Python 3.12 or higher Create virtual environment\npython -m venv bedrock-slack-env # Activate (Windows) bedrock-slack-env\\Scripts\\activate # Activate (macOS/Linux) source bedrock-slack-env/bin/activate 4.2 Install Required Dependencies Install core packages\npip install boto3 slack-bolt python-dotenv requests Install AWS CDK (for infrastructure deployment)\nnpm install -g aws-cdk cdk --version 4.3 Verify Installation Test AWS SDK connection\nimport boto3 # Test Bedrock connection bedrock = boto3.client(\u0026#39;bedrock\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39;) print(\u0026#34;AWS SDK configured successfully!\u0026#34;) Step 5: Environment Verification 5.1 Complete Setup Checklist Verify all components are properly configured:\n‚úÖ Slack workspace created and accessible ‚úÖ AWS CLI configured with valid credentials ‚úÖ Bedrock models enabled (Titan + Claude 3) ‚úÖ Python environment set up with dependencies ‚úÖ AWS CDK installed and functional 5.2 Test Basic Connectivity Run this verification script:\nimport boto3 import json def verify_setup(): try: # Test AWS connection sts = boto3.client(\u0026#39;sts\u0026#39;) identity = sts.get_caller_identity() print(f\u0026#34;‚úÖ AWS Account: {identity[\u0026#39;Account\u0026#39;]}\u0026#34;) # Test Bedrock access bedrock = boto3.client(\u0026#39;bedrock\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39;) models = bedrock.list_foundation_models() print(\u0026#34;‚úÖ Bedrock access confirmed\u0026#34;) print(\u0026#34;\\nüéâ Environment setup complete!\u0026#34;) return True except Exception as e: print(f\u0026#34;‚ùå Setup error: {e}\u0026#34;) return False if __name__ == \u0026#34;__main__\u0026#34;: verify_setup() Troubleshooting Common Issues AWS CLI Issues Problem: aws: command not found Solution: Restart terminal after installation or add AWS CLI to PATH Bedrock Access Issues Problem: Model access denied Solution: Ensure you\u0026rsquo;re in us-east-1 region and have proper IAM permissions Python Environment Issues Problem: Package installation fails Solution: Ensure virtual environment is activated and pip is updated Need Help? If you encounter issues, check the Troubleshooting Guide or ask the workshop instructor.\nNext Steps With your environment properly configured, you\u0026rsquo;re ready to start building the knowledge base foundation for your AI assistant.\nContinue to: Module 3 - Knowledge Base Foundation\nSummary In this module, you have:\n‚úÖ Created a Slack workspace for testing ‚úÖ Configured AWS CLI and verified access ‚úÖ Enabled required Amazon Bedrock models ‚úÖ Set up Python development environment ‚úÖ Verified all components are working correctly Your development environment is now ready for building the Generative AI Slack Assistant!\n"
},
{
	"uri": "//localhost:1313/3-slack_app/3.1-create_slackapp/",
	"title": "Create slack app",
	"tags": [],
	"description": "",
	"content": "Create slack app Access to api.slack.com/apps. Click Create new app -\u0026gt; From scratch\nEnter your App name, Pick a workspace to develop your app in and click Create App.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/3.2-oauthpermissions/",
	"title": "Create slash commands",
	"tags": [],
	"description": "",
	"content": "Create slash commands Choose Slash Commands. Then click Create new command.\nEnter your information: command name, Request URL, Short Decription.\nNote: The current URL is only temporary, when deployed, we will use the API URL to replace it.\nCreate slash commands successfully.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/",
	"title": "Slack App Setup",
	"tags": [],
	"description": "",
	"content": "A Slack app is a tool or integration that extends the functionality of Slack: it adds new features, automates tasks, integrates with external services, or enhances the user experience. A Slack app allows you to do more within Slack than just chat. With the Slack platform, individual and enterprise developers alike can create apps that integrate directly with the tools teams already use, whether that\u0026rsquo;s connecting a CRM, managing project boards, or sending automated alerts.\nWe know our platform is deep and wide, and possibly a little intimidating as a result. It\u0026rsquo;s okay to not know where to start.\nIf you want to take it slow, this guide on designing your app is a little light reading on how to define the look and feel of your app.\nIf you\u0026rsquo;d rather stop the chitchat and get into it, build an app with the Quickstart guide. If you\u0026rsquo;re just looking to get a token to call the Web API methods, completing the first three steps of the Quickstart will get you there.\n"
},
{
	"uri": "//localhost:1313/4-security/4.1-secret-manager/",
	"title": "Secrets Manager",
	"tags": [],
	"description": "",
	"content": " Access to Secret console\nGet token:\nCreate bot-token5\nGet Signing secret\nCreate signing-secret5\n"
},
{
	"uri": "//localhost:1313/4-security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "Security is a foundational element of this chatbot system, and it is implemented through a dual-layered approach that combines the strengths of AWS Secrets Manager and AWS Systems Manager Parameter Store to ensure the secure handling of sensitive credentials and configurations throughout the application lifecycle.\nThe primary function of AWS Secrets Manager in this project is to securely store and manage confidential credentials, particularly those required for the Slack bot\u0026rsquo;s authentication. These include the Slack Bot User OAuth Token, which is essential for programmatic access to the Slack API, and the Slack Signing Secret, which is used to validate that incoming requests to the application originate from Slack. Secrets Manager ensures these values are encrypted at rest using AWS-managed keys and supports automatic secret rotation, which enhances long-term security and reduces the risk of credential exposure. Furthermore, all access to stored secrets is tightly controlled through IAM policies, allowing only explicitly authorized roles and services to retrieve them.\nComplementing Secrets Manager is the AWS Systems Manager Parameter Store, which plays a key role in configuration management and runtime secret resolution. Instead of Lambda functions directly accessing secrets, the architecture leverages Parameter Store to create references to the secrets stored in Secrets Manager. Using the native {{resolve:secretsmanager:\u0026hellip;}} syntax, Lambda functions can dynamically fetch the latest values at execution time. This approach avoids hardcoding sensitive data in source code or environment variables, and makes deployments cleaner and safer. The use of Standard-tier parameters within SSM also ensures that the configuration layer remains cost-effective.\nThis design provides several security benefits. First, it enables a separation of concerns, where Secrets Manager handles encryption and storage, while Parameter Store manages controlled distribution. Second, credentials are never exposed in plain text or stored within the codebase, reducing attack vectors. Third, this setup supports dynamic runtime resolution, meaning credentials are always up to date when Lambda functions run. Fourth, auditing and monitoring are enabled for both Secrets Manager and Parameter Store, providing complete traceability for compliance. Lastly, it follows the principle of least privilege access, as Lambda functions require permission to read from SSM, but do not need direct access to Secrets Manager.\nTogether, these services establish a robust security architecture that ensures sensitive Slack credentials remain protected while still being seamlessly accessible to the AI chatbot application at runtime.\n"
},
{
	"uri": "//localhost:1313/3-slack_app/3.3-slash_commands/",
	"title": "Get OAuth Tokens",
	"tags": [],
	"description": "",
	"content": " Choose OAuth \u0026amp; Permissions, choose install to Your-workspace.\nYou will see OAuth Tokens.\n"
},
{
	"uri": "//localhost:1313/4-security/4.2-systems-manager/",
	"title": "Systems Manager",
	"tags": [],
	"description": "",
	"content": " Access to SSM console\nCreate new Parameter for bot-token5\nCreate new Parameter for signing-secret5\nARN\n"
},
{
	"uri": "//localhost:1313/5-opensearch/5.1-collection/",
	"title": "collection",
	"tags": [],
	"description": "",
	"content": " Access to Amazon Opensearch Service Console.\nScroll down and select Data access policies and click create access policy\nEnter Access policy name -\u0026gt; choose JSON\nEnter the following code and Create\n[ { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;index\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;index/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;/*\u0026#34;], \u0026#34;Permission\u0026#34;: [\u0026#34;aoss:*\u0026#34;] }, { \u0026#34;ResourceType\u0026#34;: \u0026#34;collection\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;collection/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;\u0026#34;], \u0026#34;Permission\u0026#34;: [\u0026#34;aoss:*\u0026#34;] } ], \u0026#34;Principal\u0026#34;: [\u0026#34;arn:aws:iam::\u0026lt;YOUR-ACCOUNT-ID\u0026gt;:root\u0026#34;] } ] Create networks policy\nName Network policy name\nEnter the following code and Create\n[ { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ResourceType\u0026#34;: \u0026#34;collection\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;collection/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;\u0026#34;] }, { \u0026#34;ResourceType\u0026#34;: \u0026#34;dashboard\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;collection/\u0026lt;YOUR-OPENSEARCH-COLLECTION-NAME\u0026gt;\u0026#34;] } ], \u0026#34;AllowFromPublic\u0026#34;: true } ] Create Collection Select Collections\nSelect Create Collection\nEnter Collection name and select Collection type: Vector search\nDefault and create. Note:\nCollection ARN Opensearch endpoint\n"
},
{
	"uri": "//localhost:1313/5-opensearch/",
	"title": "Set up opensearch",
	"tags": [],
	"description": "",
	"content": "In the context of this AI chatbot solution, Amazon OpenSearch Serverless functions as the centralized vector database, acting as the memory engine that drives the semantic retrieval capabilities of the Retrieval-Augmented Generation (RAG) workflow.\nWhen documents are first ingested into the system, they are processed and transformed into high-dimensional vector embeddings using the Amazon Titan Text Embeddings v2 model. These embeddings, which support semantic understanding across multiple languages including Vietnamese and English, are stored in a dedicated vector collection within OpenSearch Serverless. The vector collection, identified by the name slack-bedrock-vector-db, offers fully managed, serverless, and auto-scaling storage of embedding vectors, eliminating the need for manual cluster management.\nTo facilitate efficient retrieval, these stored vectors are indexed using a vector index named slack-bedrock-os-index, which enables high-performance similarity search using k-nearest neighbor (k-NN) algorithms. The OpenSearch index also retains metadata associated with the original document chunks, such as file names or section titles, ensuring that retrieval results are both contextually accurate and traceable.\nDuring a typical query interaction, when a user submits a question via Slack, the input is embedded using the same Titan v2 model. This query embedding is sent to OpenSearch, where a vector similarity search is conducted against the stored embeddings. The most relevant chunks of documents are then retrieved and returned to the RAG system to serve as context for final response generation.\nThe serverless nature of OpenSearch ensures that it scales automatically with traffic volume and operates on a pay-per-use pricing model, helping keep costs predictable and aligned with usage. The architecture requires no manual provisioning or maintenance of nodes, which significantly simplifies operations. With sub-second response times, the system supports real-time querying, providing users with instant, relevant information.\nSecurity is also embedded into the design. OpenSearch Serverless integrates with IAM-based access control and supports encryption both at rest and in transit. Through the BedrockOSSPolicyForKnowledgeBase policy, fine-grained access permissions are enforced, ensuring that only authorized components can access or query the vector database.\nUltimately, OpenSearch Serverless functions as the persistent memory layer of the AI assistant, enabling it to recall and reference vast amounts of organizational knowledge with semantic precision.\n"
},
{
	"uri": "//localhost:1313/5-opensearch/5.2-vector_index/",
	"title": "Vector index",
	"tags": [],
	"description": "",
	"content": "Create user Create a user bedrock-chatbot-deployer with policy AdministratorAccess\nCreate a tag with Key and Value are access key\nCreate index Access to Postman with: URL is Collection endpoint/\u0026lt;Index_name\u0026gt;\nAuth type: AWS Signature\nAccessKey and SecretKey\nRegion: us-east-1\nService name: aoss\nHeader:\nContent type: Application/json\nBody:\nraw\nEnter code following { \u0026#34;settings\u0026#34;: { \u0026#34;index\u0026#34;: { \u0026#34;knn\u0026#34;: true, \u0026#34;knn.algo_param.ef_search\u0026#34;: 512 } }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;bedrock-knowledge-base-default-vector\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_vector\u0026#34;, \u0026#34;dimension\u0026#34;: 1024, \u0026#34;method\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;hnsw\u0026#34;, \u0026#34;engine\u0026#34;: \u0026#34;faiss\u0026#34;, \u0026#34;space_type\u0026#34;: \u0026#34;l2\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;ef_construction\u0026#34;: 512, \u0026#34;m\u0026#34;: 16 } } }, \u0026#34;AMAZON_BEDROCK_METADATA\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: false }, \u0026#34;AMAZON_BEDROCK_TEXT_CHUNK\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;x-amz-bedrock-kb-data-source-id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;x-amz-bedrock-kb-document-page-number\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;index\u0026#34;: true }, \u0026#34;x-amz-bedrock-kb-source-uri\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;index\u0026#34;: true } } } } Click Send. Correct configuration will get response\nAn index will be created with Vector field name and Metadata\n"
},
{
	"uri": "//localhost:1313/6-bedrock_setup/",
	"title": "Bedrock setup",
	"tags": [],
	"description": "",
	"content": "In this workshop, we use a knowledge base to retrieve information provided from S3. The data from S3 is split into smaller segments (document chunks) and converted into vectors using an embeddings model. These vectors are then stored in a vector store, specifically Amazon OpenSearch Serverless.\nWhen a user submits a question, the system performs semantic search against the vector store to retrieve relevant text segments (context). This contextual information is incorporated into prompt augmentation to provide the necessary background for the Large Language Model (LLM). Finally, the LLM generates an accurate and contextually appropriate response based on the knowledge base.\nAt the heart of this AI chatbot system lies Amazon Bedrock, which serves both as the foundation model hosting platform and the managed orchestration engine for the entire RAG (Retrieve-and-Generate) pipeline. Bedrock enables the seamless combination of foundation models, vector-based retrieval, and knowledge base indexing into a unified, scalable workflow.\nThe chatbot relies on two key models provided by Bedrock. For text embedding, it utilizes Amazon Titan Text Embeddings v2, a multilingual embedding model optimized for languages like Vietnamese and English, capable of generating rich semantic representations of documents and queries. For text generation, it employs Claude 3 Haiku, a lightweight and cost-efficient model that produces natural, fluent, and contextually appropriate responses.\nThe Amazon Bedrock Knowledge Base component acts as the central RAG orchestrator. It ingests documents stored in S3 (via a designated data source), generates vector embeddings using Titan v2, and stores them in OpenSearch Serverless. At runtime, when a user submits a query, the RetrieveAndGenerate API of Bedrock is invoked. This single API call handles all the complexity: embedding the query, retrieving the most relevant document chunks via vector similarity search, passing the content to Claude 3 Haiku, and returning the generated response.\nA crucial layer of security and response control is handled by Bedrock Guardrails, configured under the name slack-bedrock-guardrail. This feature is responsible for content filtering, ensuring that responses do not contain sensitive, inappropriate, or harmful language, including topics related to violence, hate, or misconduct. It also offers protection against prompt injection attacks and enables custom error messaging, ensuring responses meet safety and compliance standards required in enterprise environments.\nFrom a technical operations perspective, Bedrock requires no infrastructure provisioning, as it offers serverless access to foundation models with automatic scalability, maintenance, and updates. This significantly reduces the burden of managing ML infrastructure. The solution is also region-optimized for ap-southeast-1, ensuring latency and cost efficiency for users in Southeast Asia.\nAccess to Bedrock is strictly controlled through IAM roles (e.g., bedrockExecutionRole), and all operations are logged to support auditing, governance, and enterprise-grade security compliance.\nIn essence, Amazon Bedrock functions as the ‚Äúbrain‚Äù of the AI chatbot, coordinating embedding, retrieval, content generation, and moderation ‚Äî all under a fully managed, secure, and scalable AI service platform.\n"
},
{
	"uri": "//localhost:1313/6-bedrock_setup/6.2-guardrails/",
	"title": "Set up guardrails",
	"tags": [],
	"description": "",
	"content": " "
},
{
	"uri": "//localhost:1313/6-bedrock_setup/6.1-model_access/",
	"title": "Set up model access",
	"tags": [],
	"description": "",
	"content": " Access to Amazon Bedrock Console.\nScroll down to the bottom of the navigation sidebar and choose Model access.\nChoose Modify model access.\nTick into the box of the Models, what you want to enable. In this workshop, i use Titan Text Embeddings V2 and Claude 3 sonnet. Scroll down to the bottom and choose Next\nBecause Titan Text Embeddings V2 was access granted before, there is only Claude 3 sonnet left. Choose Submit and after a few minutes model access will be enabled.\nFinally, both Titan Text Embeddings V2 and Claude 3 sonnet are access granted.\n"
},
{
	"uri": "//localhost:1313/7-lambda_implementation/",
	"title": "Lambda implementation",
	"tags": [],
	"description": "",
	"content": "The Lambda function in this code acts as an intermediary between Slack and Amazon Bedrock Knowledge Base. When a user enters a Slash command in Slack (e.g., /ask-ai), the event is sent to the Lambda function. Within the first 3 seconds, the Lambda must acknowledge the command using the ack function to avoid a timeout error from Slack‚Äîthis is handled by the respond_to_slack_within_3_seconds function. After that, the Lambda proceeds to process the main logic by calling Bedrock\u0026rsquo;s RetrieveAndGenerate API to retrieve relevant information and generate a response from the Knowledge Base, which is then sent back to the user in Slack.\nIn addition to responding to user queries, the Lambda function is responsible for securely initializing and managing required AWS services such as retrieving secrets from AWS Secrets Manager and parameters from SSM Parameter Store. It also sets up configuration values like model ID, knowledge base ID, and guardrail settings. This ensures that the connection between Slack and Bedrock is correctly established and that user queries are processed safely and accurately. In short, this Lambda function acts both as an orchestrator and executor of the entire interaction flow between Slack and AWS\u0026rsquo;s intelligent answering system.\n"
},
{
	"uri": "//localhost:1313/7-lambda_implementation/7.1-lambda_role/",
	"title": "Lambda role",
	"tags": [],
	"description": "",
	"content": "Create a role named BedrockExecutionRole8888: Policy AWSLambdaBasicExecutionRole:\nAdd a custom policy named BedrockExecutionPolicy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:Retrieve\u0026#34;, \u0026#34;bedrock:RetrieveAndGenerate\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;YOUR-KNOWLEDGEBASE-ARN\u0026gt;\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:GetParameter\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;\u0026lt;YOUR-SSM-ARN-1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;YOUR-SSM-ARN-1\u0026gt;\u0026#34; ] }, { \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:InvokeFunction\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;\u0026lt;YOUR-LAMBDA-FUNCTION-ARN\u0026gt;\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:ApplyGuardrail\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:us-east-1:\u0026lt;YOUR-ACCOUNT-ID\u0026gt;:guardrail/\u0026lt;YOUR-GUARDRAIL-ID\u0026gt;*\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:GetSecretValue\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;\u0026lt;YOUR-SECRET-BOT-TOKEN-ARN\u0026gt;\u0026#34;, \u0026#34;\u0026lt;YOUR-SECRET-SIGNING-SECRET-ARN\u0026gt;\u0026#34; ] } ] } "
},
{
	"uri": "//localhost:1313/6-bedrock_setup/6.3-knowledge_base/",
	"title": "Set up knowledge base",
	"tags": [],
	"description": "",
	"content": "S3 configuration Create a new bucket.\nConfigure on Bedrock Access to Amazon Bedrock Console.\nScroll down to the bottom of the navigation sidebar and choose Knowledge base.\nChoose Create and choose Knowledge Base with vector store.\nSet up knowledge base with vector store.\nEnter knowledge base name and create a new role\nData source type will be S3\nSelect S3 to store data\nSelect Embeddings model\nVector store is Amazon Opensearch Serverless\nConfiguring the Vector Store with what was created in the previous steps is the set up opensearch preparation step\nKnowledge Base will be created successfully.\nRole for knowledge base will have policies\nStore dato into Knowledge Base Upload data that chatbot will use with s3. In this session, we use postgresql-16-US.pdf\nAccess to data source in bedrock knowledge base and sync it\n"
},
{
	"uri": "//localhost:1313/7-lambda_implementation/7.2-config_code/",
	"title": "Configuration and code",
	"tags": [],
	"description": "",
	"content": " Create a lambda named BedrockKBSlackbotFunction5\nAttach role named BedrockExecutionRole8888 and Create\nUp load a file .zip named BedrockKBSlackbotFunction.zip\nChange Hanlder:\nChange Memory and Timeout\nAdd some Environment variables\n"
},
{
	"uri": "//localhost:1313/8-api_gateway/",
	"title": "Connect and chat",
	"tags": [],
	"description": "",
	"content": "API GATEWAY URL Access to API console\nCreate Rest API and name\nCreate resource named slack\nCreate another source named ask-ai\nCreate method for ask-ai\nSettings with method:\nMethod: POST Lambda proxy integration: enable Lambda function: BedrockKbSlackbotFunction5\nDeploy with new stage named prod\nGet the Invoke URL and replace it with the Request URL box in Slack. Choose Save\nAccess to Slash commands again and replace Request URL. Finally, click Save\nTESTING Use Case 1: Chatbot successfully retrieves data from the Knowledge Base\nUse Case 2: Chatbot cannot answer because the data is not present in the PDF\nUse Case 3: Chatbot cannot respond due to sensitive content blocked by AWS Bedrock Guardrails\n"
},
{
	"uri": "//localhost:1313/9-clear-resources/",
	"title": "Clear resources",
	"tags": [],
	"description": "",
	"content": "Clear resource by following steps S3\nKnowledge Base\nOpensearch Indexes\nCollections\nData access policy\nNetwork policy\nAWS Secret Managers\nAWS Systems Manager\nLambda\nAPI gateway\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]